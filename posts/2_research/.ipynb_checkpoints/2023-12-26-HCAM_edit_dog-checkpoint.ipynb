{
 "cells": [
  {
   "cell_type": "raw",
   "id": "25394d6c-d719-4db1-8369-cea8c28f15b5",
   "metadata": {
    "id": "cac470df-29e7-4148-9bbd-d8b9a32fa570",
    "tags": []
   },
   "source": [
    "---\n",
    "title: \"**[CAM]**수정본 HCAM_dog\"\n",
    "author: \"SEOYEON CHOI\"\n",
    "date: \"2023-12-26\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be66784-5447-467e-b04c-31514018f149",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b168b622-f45d-4e48-a3b6-12c182f5914e",
   "metadata": {
    "id": "e6420e49-d6db-4593-931e-18d5d88df0e4",
    "tags": []
   },
   "source": [
    "### import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fc5a2de-b4c8-4ee1-88e4-604defa42d23",
   "metadata": {
    "id": "989676d6-d597-4c4e-a89a-11e4aa915efd"
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "from fastai.vision.all import *\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ea7359d-5d88-43d5-9aac-49bb2acec4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6b2a9cc-7ed9-4fcd-8822-e4fee68f0e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6b4a516-5556-454e-8e86-232b6a09bb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import ImageDraw\n",
    "from PIL import ImageFont\n",
    "from PIL import ImageFile\n",
    "from PIL import Image\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe2f574e-9cd4-4514-ba46-2b03bc841677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "602e9cc0-699d-4cd4-83e6-30bf593cf619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpy2\n",
    "import rpy2.robjects as ro \n",
    "from rpy2.robjects.vectors import FloatVector \n",
    "from rpy2.robjects.packages import importr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "855c8b91-cfa4-457b-b4f6-06959fbfe200",
   "metadata": {
    "id": "e8148f33-c5ca-4f5f-9500-91c9569e6f62"
   },
   "outputs": [],
   "source": [
    "def label_func(f):\n",
    "    if f[0].isupper():\n",
    "        return 'cat' \n",
    "    else: \n",
    "        return 'dog' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fed40320-f547-4214-b0ae-5f70c1e5f382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53199c7d-27f3-4f3d-83ac-89ea0a755bf2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4566a861-133a-45b0-8f86-61e9918f42cb",
   "metadata": {
    "id": "7c4d95af-c434-4e4f-8c86-c44e1f2ce2f9"
   },
   "source": [
    "### Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076be514-9088-4eb7-b6bd-45400a90a3c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### - 여기서는 `랜덤박스가 추가된 개/고양이 그림`에 대해 CAM을 진행한 결과를 확인함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f15c23-e8db-46ac-b787-5fe6fb190687",
   "metadata": {},
   "source": [
    "#### (1) 랜덤박스가 들어간 개 고양이 그림 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb8e4d62-12ab-4123-a053-c4d3e38573ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=Path('random_pet_one')   #랜덤박스넣은사진"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0224ae67-438a-429c-883e-bfc623123b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#7391) [Path('random_pet_one/Bombay_13.jpg'),Path('random_pet_one/beagle_193.jpg'),Path('random_pet_one/Ragdoll_8.jpg'),Path('random_pet_one/boxer_106.jpg'),Path('random_pet_one/keeshond_56.jpg'),Path('random_pet_one/american_pit_bull_terrier_162.jpg'),Path('random_pet_one/saint_bernard_136.jpg'),Path('random_pet_one/staffordshire_bull_terrier_76.jpg'),Path('random_pet_one/pug_173.jpg'),Path('random_pet_one/american_pit_bull_terrier_117.jpg')...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbbff5ed-058c-4f48-bc5f-8568c3a2615a",
   "metadata": {
    "id": "fca1bf6a-c7aa-416b-8ac1-9922e05a20da"
   },
   "outputs": [],
   "source": [
    "files=get_image_files(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a61eb85-3850-45b3-8b03-e924b0d46976",
   "metadata": {
    "id": "33ffcc28-46b8-4dbe-b331-86372312177d"
   },
   "outputs": [],
   "source": [
    "dls=ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(512)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8408df-9b12-4146-b936-2175ba864791",
   "metadata": {},
   "source": [
    "#### (2) 학습 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370c57a0-a8ac-4737-947a-25e8249fa20f",
   "metadata": {
    "id": "8c83bf24-f52c-4467-a042-e6467d102a3a",
    "outputId": "3c87c3dd-6d54-492e-c9b9-5cd814d5b23b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csy/anaconda3/envs/temp_csy/lib/python3.8/site-packages/fastai/vision/learner.py:288: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code\n",
      "  warn(\"`cnn_learner` has been renamed to `vision_learner` -- please update your code\")\n",
      "/home/csy/anaconda3/envs/temp_csy/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/csy/anaconda3/envs/temp_csy/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='16' class='' max='92' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      17.39% [16/92 00:07&lt;00:37 0.8271]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrnr=cnn_learner(dls,resnet34,metrics=error_rate)\n",
    "lrnr.fine_tune(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d21b3d4-aa24-4de3-b476-80695ea66ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "net1=lrnr.model[0]\n",
    "net2=lrnr.model[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c0a3ec-b0a7-4cc9-807f-8a14d3a9f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = torch.nn.Sequential(\n",
    "    torch.nn.AdaptiveAvgPool2d(output_size=1), \n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(512,out_features=2,bias=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7bbf18-ffc1-4c7b-a344-0d86a0b8f52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "net=torch.nn.Sequential(net1,net2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69bffeb-a78a-4af3-84b3-cb4808285d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrnr2=Learner(dls,net,metrics=accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8472fe0f-74a9-4083-9411-4183abcf721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrnr2.fine_tune(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b9cc13-27ba-4498-8a89-258459491364",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ea2185-e0de-4460-88a3-be5516dcd9ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd01baf2-2882-4891-adb1-822bea55a0ef",
   "metadata": {},
   "source": [
    "# DOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ef5ca3-5a2a-4d40-b1ea-0dc3331235e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dog, = first(dls.test_dl([PILImage.create(get_image_files(path)[12])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66569c6-20ad-436b-b962-73a950f8ad28",
   "metadata": {},
   "outputs": [],
   "source": [
    "camimg = torch.einsum('ij,jkl -> ikl', net2[2].weight, net1(x_dog).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb463ff3-4700-4e42-a90b-834f81e16d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ebayesthresh = importr('EbayesThresh').ebayesthresh\n",
    "\n",
    "power_threshed=np.array(ebayesthresh(FloatVector(torch.tensor(camimg[0].detach().reshape(-1))**2)))\n",
    "ybar_threshed = np.where(power_threshed>1600,torch.tensor(camimg[0].detach().reshape(-1)).to('cpu'),0)\n",
    "ybar_threshed = torch.tensor(ybar_threshed.reshape(16,16))\n",
    "\n",
    "power_threshed2=np.array(ebayesthresh(FloatVector(torch.tensor(camimg[1].detach().reshape(-1))**2)))\n",
    "ybar_threshed2 = np.where(power_threshed2>2100,torch.tensor(camimg[1].detach().reshape(-1)).to('cpu'),0)\n",
    "ybar_threshed2 = torch.tensor(ybar_threshed2.reshape(16,16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af07eb66-1d3b-4e5d-9f39-67902c57b19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3) \n",
    "# \n",
    "dls.train.decode((x_dog,))[0].squeeze().show(ax=ax1)\n",
    "ax1.set_title(\"Input image\")\n",
    "# \n",
    "dls.train.decode((x_dog,))[0].squeeze().show(ax=ax2)\n",
    "ax2.imshow((ybar_threshed).to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax2.set_title(\"CAT PART\")\n",
    "#\n",
    "dls.train.decode((x_dog,))[0].squeeze().show(ax=ax3)\n",
    "ax3.imshow((ybar_threshed2).to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax3.set_title(\"DOG PART\")\n",
    "#\n",
    "fig.set_figwidth(12)            \n",
    "fig.set_figheight(12)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71691347-86d7-4d41-95e4-2203072682d6",
   "metadata": {},
   "source": [
    "- 판단 근거가 강할 수록 파란색 -> 보라색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3e1899-d16c-4c43-88fa-dfd1b3b8f052",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = net(x_dog).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa0867b-e3e4-46a2-9b18-f92a1b9be1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(a)/ (np.exp(a)+np.exp(b)) ,  np.exp(b)/ (np.exp(a)+np.exp(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67516d9-54b4-4b16-87e6-49a3e44a458f",
   "metadata": {},
   "source": [
    "## mode 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce272509-7a74-4176-a9ea-4dbdece498da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test=camimg_o[0]-torch.min(camimg_o[0])\n",
    "A1=torch.exp(-0.05*(ybar_threshed2))\n",
    "A2 = 1 - A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cb07b3-6adc-4000-b6cc-400f89c57a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2) \n",
    "# \n",
    "dls.train.decode((x_dog,))[0].squeeze().show(ax=ax1)\n",
    "ax1.imshow(A2.data.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax1.set_title(\"MODE1 WEIGHTT\")\n",
    "#\n",
    "dls.train.decode((x_dog,))[0].squeeze().show(ax=ax2)\n",
    "ax2.imshow(A1.data.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax2.set_title(\"MODE1 RES WEIGHT\")\n",
    "#\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c6a7b5-0746-4cc4-a1bb-239e9f51bd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode 1 res\n",
    "X1=np.array(A1.to(\"cpu\").detach(),dtype=np.float32)\n",
    "Y1=torch.Tensor(cv2.resize(X1,(512,512),interpolation=cv2.INTER_LINEAR))\n",
    "x1=x_dog.squeeze().to('cpu')*Y1-torch.min(x_dog.squeeze().to('cpu'))*Y1\n",
    "\n",
    "# mode 1\n",
    "X12=np.array(A2.to(\"cpu\").detach(),dtype=np.float32)\n",
    "Y12=torch.Tensor(cv2.resize(X12,(512,512),interpolation=cv2.INTER_LINEAR))\n",
    "x12=x_dog.squeeze().to('cpu')*Y12-torch.min(x_dog.squeeze().to('cpu'))*Y12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8bf55c-3e04-404d-995c-a67e226b8deb",
   "metadata": {},
   "source": [
    "`-` 1st CAM 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62880983-c305-412b-93ed-02660d8f0547",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(1,1) \n",
    "dls.train.decode((x_dog,))[0].squeeze().show(ax=ax1)\n",
    "ax1.set_title(\"ORIGINAL\")\n",
    "fig.set_figwidth(4)            \n",
    "fig.set_figheight(4)\n",
    "fig.tight_layout()\n",
    "#\n",
    "fig, (ax1, ax2) = plt.subplots(1,2) \n",
    "(x12*0.3).squeeze().show(ax=ax1)  #MODE1\n",
    "(x1*0.3).squeeze().show(ax=ax2)  #MODE1_res\n",
    "ax1.set_title(\"MODE1\")\n",
    "ax2.set_title(\"MODE1 RES\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55cc96c-8974-40cc-b049-572ab0fe1c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = x1.reshape(1,3,512,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a022d8cc-9fe7-4f94-9c0b-6052fc7323d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "net1.to('cpu')\n",
    "net2.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce8e708-9f43-4de8-8e8b-b33b62fe8cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "camimg1 = torch.einsum('ij,jkl -> ikl', net2[2].weight, net1(x1).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa47040-bfd6-41e2-ad54-0e5158a8d47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_threshed3=np.array(ebayesthresh(FloatVector(torch.tensor(camimg1[0].detach().reshape(-1))**2)))\n",
    "ybar_threshed3 = np.where(power_threshed3>10,torch.tensor(camimg1[0].detach().reshape(-1)),0)\n",
    "ybar_threshed3 = torch.tensor(ybar_threshed3.reshape(16,16))\n",
    "\n",
    "power_threshed4=np.array(ebayesthresh(FloatVector(torch.tensor(camimg1[1].detach().reshape(-1))**2)))\n",
    "ybar_threshed4 = np.where(power_threshed4>10,torch.tensor(camimg1[1].detach().reshape(-1)),0)\n",
    "ybar_threshed4 = torch.tensor(ybar_threshed4.reshape(16,16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199120db-d78c-4487-bad2-c10b425bc23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1,b1 = net(x1).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75b0377-b26d-49ce-9b5b-e8236ada134e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(a1)/ (np.exp(a1)+np.exp(b1)) ,  np.exp(b1)/ (np.exp(a1)+np.exp(b1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47e1c4b-a619-4b4a-ac11-343f46d02a38",
   "metadata": {},
   "source": [
    "`-` mode1 res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6273442-18fc-4dfe-b3c4-a0e40564e363",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2) \n",
    "# \n",
    "(x1*0.25).squeeze().show(ax=ax1)\n",
    "ax1.imshow(ybar_threshed3,alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax1.set_title(\"CAT PART\")\n",
    "#\n",
    "(x1*0.25).squeeze().show(ax=ax2)\n",
    "ax2.imshow(ybar_threshed4,alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax2.set_title(\"DOG PART\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5da2a13-f952-42d9-a9a8-d5bf56fed2c1",
   "metadata": {},
   "source": [
    "`-` 첫번째 CAM 결과와 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b44a4ad-09f8-4ba6-98d9-b515c53861b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2) \n",
    "# \n",
    "dls.train.decode((x_dog,))[0].squeeze().show(ax=ax1)\n",
    "ax1.imshow(ybar_threshed2,alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax1.set_title(\"1ST CAM\")\n",
    "#\n",
    "dls.train.decode((x_dog,))[0].squeeze().show(ax=ax2)\n",
    "ax2.imshow(ybar_threshed4,alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax2.set_title(\"2ND CAM\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac65f82e-97f0-4a90-b21b-381e4d2ff57f",
   "metadata": {},
   "source": [
    "`-` 2nd CAM 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6201dda8-dd99-40b3-bdb0-a6d3870891fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test1=camimg1[1]-torch.min(camimg1[1])\n",
    "A3 = torch.exp(-0.05*(ybar_threshed4))\n",
    "A4 = 1 - A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c408d7-82a4-4f1f-8920-45a3553be37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2) \n",
    "# \n",
    "x1.squeeze().show(ax=ax2)\n",
    "dls.train.decode((x1,))[0].squeeze().show(ax=ax1)\n",
    "ax1.imshow(A3.data.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax1.set_title(\"MODE2 RES WEIGHT\")\n",
    "#\n",
    "x1.squeeze().show(ax=ax2)\n",
    "dls.train.decode((x1,))[0].squeeze().show(ax=ax2)\n",
    "ax2.imshow(A4.data.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax2.set_title(\"MODE2 WEIGHT\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce580315-d4d6-4040-80eb-004ddaae21b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res \n",
    "X2=np.array(A3.to(\"cpu\").detach(),dtype=np.float32)\n",
    "Y2=torch.Tensor(cv2.resize(X2,(512,512),interpolation=cv2.INTER_LINEAR))\n",
    "x2=(x1*0.2)*Y2-torch.min((x1*0.2)*Y2)\n",
    "#\n",
    "X22=np.array(A4.to(\"cpu\").detach(),dtype=np.float32)\n",
    "Y22=torch.Tensor(cv2.resize(X22,(512,512),interpolation=cv2.INTER_LINEAR))\n",
    "x22=(x1*0.2)*Y22-torch.min((x1*0.2)*Y22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8564cb4f-6f5c-40ea-bda3-71b7d435b858",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(1,1) \n",
    "dls.train.decode((x_dog,))[0].squeeze().show(ax=ax1)\n",
    "ax1.set_title(\"ORIGINAL\")\n",
    "fig.set_figwidth(4)            \n",
    "fig.set_figheight(4)\n",
    "fig.tight_layout()\n",
    "#\n",
    "fig, (ax1, ax2) = plt.subplots(1,2) \n",
    "(x12*0.3).squeeze().show(ax=ax1)  #MODE1\n",
    "(x1*0.2).squeeze().show(ax=ax2)  #MODE1_res\n",
    "ax1.set_title(\"MODE1\")\n",
    "ax2.set_title(\"MODE1 RES\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()\n",
    "#\n",
    "fig, (ax1, ax2) = plt.subplots(1,2) \n",
    "(x22*3).squeeze().show(ax=ax1)  #MODE2\n",
    "(x2).squeeze().show(ax=ax2)  #MODE2_res\n",
    "ax1.set_title(\"MODE2\")\n",
    "ax2.set_title(\"MODE2 RES\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85216048-cda8-49ec-bfa3-7cd1163d7dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = x2.reshape(1,3,512,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f7b839-2ff1-49da-8060-1f385cc98754",
   "metadata": {},
   "outputs": [],
   "source": [
    "net1.to('cpu')\n",
    "net2.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a834b07-0222-4ac9-a357-6e64df384241",
   "metadata": {},
   "outputs": [],
   "source": [
    "camimg2 = torch.einsum('ij,jkl -> ikl', net2[2].weight, net1(x2).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465cfd31-7150-4bb3-b41e-06610f4853b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_threshed5=np.array(ebayesthresh(FloatVector(torch.tensor(camimg2[0].detach().reshape(-1))**2)))\n",
    "ybar_threshed5 = np.where(power_threshed5>4,torch.tensor(camimg2[0].detach().reshape(-1)),0)\n",
    "ybar_threshed5 = torch.tensor(ybar_threshed5.reshape(16,16))\n",
    "\n",
    "power_threshed6=np.array(ebayesthresh(FloatVector(torch.tensor(camimg2[1].detach().reshape(-1))**2)))\n",
    "ybar_threshed6 = np.where(power_threshed6>4,torch.tensor(camimg1[1].detach().reshape(-1)),0)\n",
    "ybar_threshed6 = torch.tensor(ybar_threshed6.reshape(16,16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc49161a-e19a-44a5-ba29-e5217dd77777",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2,b2 = net(x2).tolist()[0]\n",
    "np.exp(a2)/(np.exp(a2)+np.exp(b2)), np.exp(b2)/(np.exp(a2)+np.exp(b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972dfb62-fa51-4d8d-9394-08211be89879",
   "metadata": {},
   "source": [
    "`-` mode2 res 에 CAM 결과 올리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0aa961-54ff-4377-9f8e-0d30d52827c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2) \n",
    "#\n",
    "x2.squeeze().show(ax=ax1)\n",
    "ax1.imshow(ybar_threshed5,alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax1.set_title(\"CAT PART\")\n",
    "#\n",
    "x2.squeeze().show(ax=ax2)\n",
    "ax2.imshow(ybar_threshed6,alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax2.set_title(\"DOG PART\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebb616a-2a14-4752-8b96-9a5124567c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3) \n",
    "# \n",
    "dls.train.decode((x_dog,))[0].squeeze().show(ax=ax1)\n",
    "ax1.imshow(ybar_threshed2,alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax1.set_title(\"1ST CAM\")\n",
    "#\n",
    "dls.train.decode((x_dog,))[0].squeeze().show(ax=ax2)\n",
    "ax2.imshow(ybar_threshed4,alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax2.set_title(\"2ND CAM\")\n",
    "#\n",
    "dls.train.decode((x_dog,))[0].squeeze().show(ax=ax3)\n",
    "ax3.imshow(ybar_threshed6,alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax3.set_title(\"3RD CAM\")\n",
    "fig.set_figwidth(12)            \n",
    "fig.set_figheight(12)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc90187-0dc0-497c-8050-c69ed0f4dbd5",
   "metadata": {},
   "source": [
    "## mode 3 만들기 더이상 분리되지 않는 듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9629abc-6c7d-4be3-925b-45e467ae1e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test2=camimg2[1]-torch.min(camimg2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f435113-cece-4b30-90ee-d8894a634600",
   "metadata": {},
   "outputs": [],
   "source": [
    "A5 = torch.exp(-0.05*(ybar_threshed6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a020c4-5130-483d-94c7-d73d59c35aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "A6 = 1 - A5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea86953-ac13-47ff-ac00-3d47b83654a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2) \n",
    "#\n",
    "x2.squeeze().show(ax=ax1)\n",
    "ax1.imshow(ybar_threshed5,alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax1.set_title(\"CAT PART\")\n",
    "#\n",
    "x2.squeeze().show(ax=ax2)\n",
    "ax2.imshow(ybar_threshed6,alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax2.set_title(\"DOG PART\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186cea02-7b1e-47e1-aeda-132cde44be43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mode 3 res\n",
    "X3=np.array(A5.to(\"cpu\").detach(),dtype=np.float32)\n",
    "Y3=torch.Tensor(cv2.resize(X3,(512,512),interpolation=cv2.INTER_LINEAR))\n",
    "x3=x2*Y3-torch.min(x2*Y3)\n",
    "# mode 3\n",
    "X32=np.array(A6.to(\"cpu\").detach(),dtype=np.float32)\n",
    "Y32=torch.Tensor(cv2.resize(X32,(512,512),interpolation=cv2.INTER_LINEAR))\n",
    "x32=x2*Y32-torch.min(x2*Y32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361ae8c7-6222-47ce-9e33-5788bef18a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(1,1) \n",
    "dls.train.decode((x_dog,))[0].squeeze().show(ax=ax1)\n",
    "ax1.set_title(\"ORIGINAL\")\n",
    "fig.set_figwidth(4)            \n",
    "fig.set_figheight(4)\n",
    "fig.tight_layout()\n",
    "#\n",
    "fig, (ax1, ax2) = plt.subplots(1,2) \n",
    "(x12*0.3).squeeze().show(ax=ax1)  #MODE1\n",
    "(x1*0.2).squeeze().show(ax=ax2)  #MODE1_res\n",
    "ax1.set_title(\"MODE1\")\n",
    "ax2.set_title(\"MODE1 RES\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()\n",
    "#\n",
    "fig, (ax1, ax2) = plt.subplots(1,2) \n",
    "(x22*4).squeeze().show(ax=ax1)  #MODE2\n",
    "(x2).squeeze().show(ax=ax2)  #MODE2_res\n",
    "ax1.set_title(\"MODE2\")\n",
    "ax2.set_title(\"MODE2 RES\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()\n",
    "#\n",
    "fig, (ax1, ax2) = plt.subplots(1,2) \n",
    "(x32*8).squeeze().show(ax=ax1)  #MODE3\n",
    "(x3).squeeze().show(ax=ax2)  #MODE3_res\n",
    "ax1.set_title(\"MODE3\")\n",
    "ax2.set_title(\"MODE3 RES\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()\n",
    "\n",
    "with open('hcam_dog_plt.pkl', 'wb') as file:\n",
    "    pickle.dump(fig, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea01406d-18d6-493a-914b-aacd46908b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hcam_dog_plt.pkl', 'rb') as file:\n",
    "    hcam_dog_plt = pickle.load(file)\n",
    "plt.show(hcam_dog_plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c177497-dd02-4b7e-b45c-5d404b569ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(1,1) \n",
    "(x12*0.3).squeeze().show(ax=ax1)  #MODE1\n",
    "ax1.set_title(\"MODE1\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()\n",
    "\n",
    "with open('hcam_dog_plt_mode1.pkl', 'wb') as file:\n",
    "    pickle.dump(fig, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd1fb58-13fd-417a-8fcd-41b3343cdb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hcam_dog_plt_mode1.pkl', 'rb') as file:\n",
    "    hcam_dog_plt_mode1 = pickle.load(file)\n",
    "plt.show(hcam_dog_plt_mode1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539c252f-08e4-4e86-8b24-ec2a3a641d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(1,1) \n",
    "(x12*0.3 + x22*4).squeeze().show(ax=ax1)  #MODE1+MODE2\n",
    "ax1.set_title(\"MODE1+MODE2\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()\n",
    "\n",
    "with open('hcam_dog_plt_mode2.pkl', 'wb') as file:\n",
    "    pickle.dump(fig, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93ec5cc-27aa-4603-b32d-7d5d1d72ff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hcam_dog_plt_mode2.pkl', 'rb') as file:\n",
    "    hcam_dog_plt_mode2 = pickle.load(file)\n",
    "plt.show(hcam_dog_plt_mode2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e389db7-4ac2-4aae-8468-7743b6f108b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(1,1) \n",
    "(x12*0.3 + x22*4 + x32*2).squeeze().show(ax=ax1)  #MODE1+MODE2+MODE3\n",
    "ax1.set_title(\"MODE3+MODE2+MODE3\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()\n",
    "\n",
    "with open('hcam_dog_plt_mode3.pkl', 'wb') as file:\n",
    "    pickle.dump(fig, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85420bb8-3fa4-465c-a04c-6b26a944f58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hcam_dog_plt_mode3.pkl', 'rb') as file:\n",
    "    hcam_dog_plt_mode3 = pickle.load(file)\n",
    "plt.show(hcam_dog_plt_mode3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc64e76-93d3-45d1-a801-cd3a6447abea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48274c97-4a06-4f71-b3ed-8c0682e198ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
